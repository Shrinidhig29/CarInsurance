{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fa92ff6e1e5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# import the modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "- Load the train data and using all your knowledge try to explore the different statistical properties of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "df_train = pd.read_csv(r\"C:\\Users\\nidhi\\Desktop\\Hackton\\Car Insurance Claim\\data\\train.csv\")\n",
    "df_test = pd.read_csv(r\"C:\\Users\\nidhi\\Desktop\\Hackton\\Car Insurance Claim\\data\\test.csv\")\n",
    "\n",
    "df_train.drop(columns='Unnamed: 0',inplace=True)\n",
    "df_test.drop(columns='Unnamed: 0',inplace=True)\n",
    "\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var = df_train.select_dtypes(exclude=\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_var.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_train['GENDER'].unique())\n",
    "print(df_train['EDUCATION'].unique())\n",
    "print(df_train['OCCUPATION'].unique())\n",
    "print(df_train['BLUEBOOK'].unique())\n",
    "print(df_train['CAR_TYPE'].unique())\n",
    "print(df_train['RED_CAR'].unique())\n",
    "print(df_train['REVOKED'].unique())\n",
    "print(df_train['PARENT1'].unique())\n",
    "print(df_train['MSTATUS'].unique())\n",
    "print(df_train['CAR_USE'].unique())\n",
    "# print(df['CLM_AMT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['INCOME'].value_counts().sum())\n",
    "print(df_train['HOME_VAL'].value_counts().sum())\n",
    "print(df_train['OLDCLAIM'].value_counts().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERSION\n",
    "* INCOME\n",
    "* HOME_AVL\n",
    "* OLDCLAIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clean the dollar sign from the currency column & convert it to float.\n",
    "    \n",
    "def clean_dollar(df_train, col):\n",
    "    \"\"\"Removes \"$\" sign from a column & converts it to float.\n",
    "    \n",
    "    This function accepts a dataframe and columns with $ sign to be converted to float.\n",
    "        \n",
    "    Keyword arguments:\n",
    "    df -- pandas dataframe for which we want to encode the columns\n",
    "    col -- list of columns that is to be converted\n",
    "    \n",
    "    Returns:\n",
    "    df -- the converted dataframe\n",
    "    \"\"\"\n",
    "    df_train[col]=df_train[col].str.replace('$','')\n",
    "    df_train[col]=df_train[col].astype(str).str.replace(\",\", \"\")\n",
    "#     df_train[col]=df_train[col].astype(int)\n",
    "#     df['Unique ID'] = df['Unique ID'].astype(int)\n",
    "    return df_train[col]    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing spaces with 'NaN' in test dataset\n",
    "df_train['INCOME'].replace(' ',np.NaN, inplace=True)\n",
    "df_train['OCCUPATION'].replace(' ',np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing or null values\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dollar(df_train, 'INCOME')\n",
    "clean_dollar(df_train, 'HOME_VAL')\n",
    "clean_dollar(df_train, 'OLDCLAIM')\n",
    "clean_dollar(df_train, 'CLM_AMT')\n",
    "clean_dollar(df_train, 'BLUEBOOK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_var = df_train.select_dtypes(include=\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_var.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_train['KIDSDRIV'].unique())\n",
    "print(df_train['AGE'].unique())\n",
    "print(df_train['HOMEKIDS'].unique())\n",
    "print(df_train['YOJ'].unique())\n",
    "print(df_train['TRAVTIME'].unique())\n",
    "print(df_train['TIF'].unique())\n",
    "print(df_train['CLM_FREQ'].unique())\n",
    "print(df_train['MVR_PTS'].unique())\n",
    "print(df_train['CAR_AGE'].unique())\n",
    "print(df_train['CLAIM_FLAG'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to treat missing values\n",
    "\n",
    "def treat_null_values(df, method):\n",
    "    \"\"\"Treats the missing values in the dataframe.\n",
    "    \n",
    "    This function accepts a dataframe and the method to treat the missing value.\n",
    "    Depending on the method passed, impute/drop the missing values.\n",
    "        \n",
    "    Keyword arguments:\n",
    "    df -- pandas dataframe for which we want to treat the missing values\n",
    "    method -- method to treat the missing values\n",
    "    \"\"\"\n",
    "    if (method=='drop'):\n",
    "        df.dropna(inplace=True)\n",
    "    elif (method=='ffill'):\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "    else:\n",
    "        df.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treat_null_values(df_train, method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing values(columns) :  \n",
    "* YOJ \n",
    "* INCOME\n",
    "* HOME_AVL\n",
    "* OCCUPATION\n",
    "* CAR_AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.get_dummies(df_train, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "- Check for the categorical & continuous features. \n",
    "- Check out the best plots for plotting between categorical target and continuous features and try making some inferences from these plots.\n",
    "- Clean the data, apply some data preprocessing and engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data, remove correlated features and apply some data preprocessing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Code ends here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building\n",
    "\n",
    "- Separate the features and target.\n",
    "- Now let's come to the actual task, using Decision Tree, predict the `paid.back.loan`. Use different techniques you have learned to imporove the performance of the model.\n",
    "- Try improving upon the `accuracy_score` ([Accuracy Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Starts here\n",
    "\n",
    "\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on the test data and creating the sample submission file.\n",
    "\n",
    "- Load the test data and store the `Id` column in a separate variable.\n",
    "- Perform the same operations on the test data that you have performed on the train data.\n",
    "- Create the submission file as a `csv` file consisting of the `Id` column from the test data and your prediction as the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Starts here\n",
    "\n",
    "\n",
    "\n",
    "# Code ends here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
